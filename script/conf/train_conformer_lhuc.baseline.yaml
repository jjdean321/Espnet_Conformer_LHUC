# network architecture
# encoder related
elayers: 12
eunits: 2048
# decoder related
dlayers: 6
dunits: 2048
# attention related
adim: 256
aheads: 4

# hybrid CTC/attention
mtlalpha: 0.2

# label smoothing
lsm-weight: 0.1

# minibatch related
batch-size: 64  # 32
maxlen-in: 512  # if input length  > maxlen-in, batchsize is automatically reduced
maxlen-out: 150 # if output length > maxlen-out, batchsize is automatically reduced

# optimization related
sortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs
opt: noam
accum-grad: 8
grad-clip: 5
patience: 0
epochs: 100
dropout-rate: 0.1

# transformer specific setting
backend: pytorch
model-module: "espnet.nets.pytorch_backend.e2e_asr_conformer_lhuc:E2E"
transformer-input-layer: conv2d     # encoder architecture type
transformer-lr: 5.0
transformer-warmup-steps: 25000
transformer-attn-dropout-rate: 0.0
transformer-length-normalized-loss: false
transformer-init: pytorch

# conformer specific setting
transformer-encoder-pos-enc-layer-type: rel_pos
transformer-encoder-selfattn-layer-type: rel_selfattn
transformer-encoder-activation-type: swish
macaron-style: true
use-cnn-module: true
cnn-module-kernel: 31

# lhuc layers
data-use-spkid: True # whether data contains the speaker id, lhuc must set this to be true. since we add the one more dimension for spkid, but do not want to apply any transform for the last dimension of the input features
# speaker-adaptation: sat-scratch #sat-scratch #test-adapt   # test-adapt, sat, sat-scratch 
# lhuc-layers: "lhuc-conv2d-2" # "lhuc-conv2d-linear" # "lhuc-conv2d-2" #"lhuc-conv2d-2,lhuc-conv2d-linear"
# model-si-path: "/project_bdda6/bdda/jjdeng/espnet/egs/swbd/asr2/exp/1024_conformer/train_nodup_trim_conformer_bs64_baseline_train_pytorch_conformer_lr5_specaug/results/model.acc.best"  #"/project_bdda6/bdda/jjdeng/espnet/egs/swbd/asr2/exp/eval2000_pytorch_transformer_baseline_lhuc_debug_1ep/results/model.acc.best"
